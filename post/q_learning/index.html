<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Q-Learning - 막돼먹은 엔지니어의 머신런닝 개발</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<meta property="og:title" content="Q-Learning" />
<meta property="og:description" content="강화 학습(Reinforcement Learning)에서 사용하는 Q-Learning에 대해서 알아 보고 Gym에서 제공하는 문제를 해결하기 위한 알고리듬을 만들어 보자." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/q_learning/" />
<meta property="article:published_time" content="2019-09-26T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-09-26T00:00:00+00:00" />

	
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="/" title="막돼먹은 엔지니어의 머신런닝 개발" rel="home">
				<div class="logo__title">막돼먹은 엔지니어의 머신런닝 개발</div>
				<div class="logo__tagline">딥러닝 모델링</div>
			</a>
		</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Q-Learning</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2019-09-26T00:00:00">2019-09-26</time>
</div>

<div class="meta__item-categories meta__item">
	<svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg>
	<span class="meta__text"><a class="meta__link" href="/categories/reinforcement-learning" rel="category">Reinforcement Learning</a>, <a class="meta__link" href="/categories/q-learning" rel="category">Q-Learning</a></span>
</div>
</div>
		</header>
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#문제-분석-problem-anaysis">문제 분석 (Problem Anaysis)</a></li>
<li><a href="#환경-environment">환경 (Environment)</a></li>
<li><a href="#q-learning">Q-Learning</a>
<ul>
<li><a href="#sarsa-greedy">SARSA + greedy</a></li>
<li><a href="#sarsa-𝜀-greedy">SARSA + 𝜀-greedy</a></li>
<li><a href="#q-learning-1">Q-Learning</a>
<ul>
<li><a href="#target">Target</a></li>
<li><a href="#error-델타">Error (델타)</a></li>
</ul></li>
</ul></li>
<li><a href="#학습-learning">학습 (Learning)</a></li>
<li><a href="#해결-solution">해결 (Solution)</a></li>
</ul></li>
</ul>
</nav>
	</div>
</div>
<div class="content post__content clearfix">
			<p>강화 학습(Reinforcement Learning)에서 사용하는 Q-Learning에 대해서 알아 보고 Gym에서 제공하는 문제를 해결하기 위한 알고리듬을 만들어 보자.</p>

<p>실제로 돌려 보고 싶으면 구글 코랩으로 ~</p>

<p><a href="https://colab.research.google.com/github/skettee/notebooks/blob/master/q_learning.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></p>

<p>👤 상사</p>

<blockquote>
<p>SARSA에서 마지막 A를 수행을 안해도 되는 방법이 있다고 하네?<br />
아래 체육관(Gym)에 가서<br />
&lsquo;얼음 호수8X8&rsquo; 문제를 그걸로 풀어 보게</p>

<p><a href="https://gym.openai.com/envs/FrozenLake8x8-v0/">https://gym.openai.com/envs/FrozenLake8x8-v0/</a></p>
</blockquote>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>네~ 네~</p>
</blockquote>

<h2 id="문제-분석-problem-anaysis">문제 분석 (Problem Anaysis)</h2>

<p>&lsquo;얼음 호수8X8&rsquo;은 살사(SARSA)에서 돌려 보았던 문제다. 이번에는 미끄러 지는 것을 추가해 보자!</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> gym
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> time <span style="color:#f92672">import</span> sleep
<span style="color:#f92672">from</span> IPython.display <span style="color:#f92672">import</span> display, clear_output, Pretty</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#</span>
<span style="color:#75715e"># Environment</span>
<span style="color:#75715e">#</span>
env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#39;FrozenLake8x8-v0&#39;</span>)
state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()

<span style="color:#75715e"># Initial world display</span>
world <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>render(mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ansi&#39;</span>)
display(Pretty(world))
sleep(<span style="color:#ae81ff">0.5</span>)

<span style="color:#75715e">#</span>
<span style="color:#75715e"># Agent</span>
<span style="color:#75715e">#</span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">100</span>):
    action <span style="color:#f92672">=</span>env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>sample()
    next_state, reward, done, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)    
    state <span style="color:#f92672">=</span> next_state
    
    <span style="color:#75715e"># updated world display</span>
    world <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>render(mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ansi&#39;</span>)
    clear_output(wait<span style="color:#f92672">=</span>True)
    display(Pretty(world))
    sleep(<span style="color:#ae81ff">0.5</span>)
    
    <span style="color:#66d9ef">if</span> done: <span style="color:#75715e"># an episode finished</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Episode finished after {} timesteps&#34;</span><span style="color:#f92672">.</span>format(step<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>))
        <span style="color:#66d9ef">break</span></code></pre></div>
<pre><code>  (Up)
SFFFFFFF
FFFFFFFF
FFF[41mH[0mFFFF
FFFFFHFF
FFFHFFFF
FHHFFFHF
FHFFHFHF
FFFHFFFG



Episode finished after 43 timesteps
</code></pre>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>가끔 액션과는 다른 방향으로 가버린다.<br />
과연  미끄러져서 구멍에 빠지지 않고<br />
목표점에 도달할 수 있을까?</p>
</blockquote>

<h2 id="환경-environment">환경 (Environment)</h2>

<p>&lsquo;얼음호수8X8&rsquo; 세계의 환경은 64개의 상태(State)와 4개의 액션(Action)으로 구성 되어 있다.  그리고 아래를 보자</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">env<span style="color:#f92672">.</span>P[<span style="color:#ae81ff">55</span>]</code></pre></div>
<pre><code>{0: [(0.3333333333333333, 47, 0.0, False),
  (0.3333333333333333, 54, 0.0, True),
  (0.3333333333333333, 63, 1.0, True)],
 1: [(0.3333333333333333, 54, 0.0, True),
  (0.3333333333333333, 63, 1.0, True),
  (0.3333333333333333, 55, 0.0, False)],
 2: [(0.3333333333333333, 63, 1.0, True),
  (0.3333333333333333, 55, 0.0, False),
  (0.3333333333333333, 47, 0.0, False)],
 3: [(0.3333333333333333, 55, 0.0, False),
  (0.3333333333333333, 47, 0.0, False),
  (0.3333333333333333, 54, 0.0, True)]}
</code></pre>

<p>해석을 해보면 다음과 같다.</p>

<blockquote>
<p>55번 상태(state)에서,<br />
왼쪽으로 이동하라는 액션을 주면, 1/3의 확률로 위로 이동하고,  1/3의 확률로 왼쪽으로 이동하고, 1/3의 확률로 아래로 이동한다.<br />
아래로 이동하라는 액션을 주면, 1/3의 확률로 왼쪽으로 이동하고, 1/3의 확률로 아래로 이동하고, 1/3의 확률로 그 자리에 있는다.<br />
오른쪽으로 이동하라는 액션을 주면, 1/3의 확률로 아래로 이동하고, 1/3의 확률로 그자리에 있고, 1/3의 확률로 위로 이동한다.<br />
아래로 이동하라는 액션을 주면, 1/3의 확률로 그 자리에 있고, 1/3의 확률로 위로 이동하고, 1/3의 확률로 왼쪽으로 이동한다.</p>
</blockquote>

<p>이렇게 미끄러짐이 추가 되면 1/3의 확률로 정상적인 액션을 수행한다.</p>

<h2 id="q-learning">Q-Learning</h2>

<p>Q-Learning 의 원리는 서로 다른 정책(policy)으로 학습 시킨 데이터를 섞어도 최적화가 가능하다는 것이다.</p>

<h3 id="sarsa-greedy">SARSA + greedy</h3>

<p>SARSA + greedy 방식으로 학습한 Q-value는 아래와 같다.</p>

<p>\(Q(S_t, A&rsquo;_t) \leftarrow Q(S_t, A&rsquo;_t) + \alpha \left( R_{t+1} + \gamma Q(S_{t+1}, A&rsquo;_{t+1}) - Q(S_t, A&rsquo;_t) \right)\)</p>

<p>greedy 방식은 다음 스텝의 액션을 선택하는 경우에 욕심쟁이(greedy)처럼 최고의 Q-value의 액션만을 선택한다. 따라서 다음 식을 만족한다.</p>

<p>\(A&rsquo;_{t+1} = \text{argmax}_{a&rsquo;} Q(S_{t+1}, a&rsquo;)\)</p>

<p>따라서 Q-value는 다음과 같다.</p>

<p>\(Q(S_t, A&rsquo;_t) \leftarrow Q(S_t, A&rsquo;_t) + \alpha \left( R_{t+1} + \gamma \max_{a&rsquo;} Q(S_{t+1}, a&rsquo;) - Q(S_t, A&rsquo;_t) \right)\)</p>

<h3 id="sarsa-𝜀-greedy">SARSA + 𝜀-greedy</h3>

<p>SARSA + 𝜀-greedy 방식으로 학습한 Q-value는 아래와 같다.</p>

<p>\(Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left( R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \right)\)</p>

<p>여기서 \(Q(S_{t+1}, A_{t+1})\) 대신에 SARSA + greedy 방식으로 만들어진 \(\max_{a&rsquo;} Q(S_{t+1}, a&rsquo;)\)을 사용한다.  그러면&hellip;</p>

<h3 id="q-learning-1">Q-Learning</h3>

<p>\(Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left( R_{t+1} + \gamma \max_{a&rsquo;} Q(S_{t+1}, a&rsquo;) - Q(S_t, A_t) \right)\)</p>

<p>\(\alpha\) : learning rate<br />
\(\gamma\) : 디스카운트 (discount factor)</p>

<h4 id="target">Target</h4>

<p>목표로 하는 값이다. 여기서 타겟은 \(R_{t+1} + \gamma \max_{a&rsquo;} Q(S_{t+1}, a&rsquo;)\)이다.</p>

<h4 id="error-델타">Error (델타)</h4>

<p>목표값과 현재값과의 차이를 \(\delta\) 라고 한다.</p>

<p>\(\delta_t = R_{t+1} + \gamma \max_{a&rsquo;} Q(S_{t+1}, a&rsquo;) - Q(S_t, A_t)\)</p>

<p>계속해서 에피소드를 실행 시키면서 \(Q(S_t, A_t)\)에다가 \(\alpha * \delta_t\) 를 업데이트 하면 결국 \(Q(s, a) \rightarrow q_*(s,a)\)가 된다.</p>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>SARSA에서 마지막 A를 수행할 필요 없이<br />
Q 테이블에서 가장 큰 수의 액션을 선택하면<br />
최적화가 가능하다는 것이 바로<br />
Q-Learning이다!</p>
</blockquote>

<h2 id="학습-learning">학습 (Learning)</h2>

<p>Q-Learning을 이용해서 최적의 Q-value를 찾아보자!</p>

<p>SARSA보다 코드가 깔끔하다!</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> tqdm <span style="color:#f92672">import</span> tqdm

num_state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>n
num_action <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n
num_episode <span style="color:#f92672">=</span> <span style="color:#ae81ff">5000</span>

<span style="color:#75715e"># Initialize Q_table </span>
Q_table <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(low<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, high<span style="color:#f92672">=</span><span style="color:#ae81ff">0.00000001</span>, size<span style="color:#f92672">=</span>(num_state, num_action))
<span style="color:#75715e"># Zero for terminate states</span>
<span style="color:#66d9ef">for</span> s <span style="color:#f92672">in</span> [<span style="color:#ae81ff">19</span>, <span style="color:#ae81ff">29</span>, <span style="color:#ae81ff">35</span>, <span style="color:#ae81ff">41</span>, <span style="color:#ae81ff">42</span>, <span style="color:#ae81ff">49</span>, <span style="color:#ae81ff">52</span>, <span style="color:#ae81ff">54</span>, <span style="color:#ae81ff">59</span>, <span style="color:#ae81ff">63</span>]:
    Q_table[s] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>

<span style="color:#75715e"># Hyper parameter</span>
epsilon <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.3</span>
alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.9</span>

<span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> tqdm(range(num_episode)):
    state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
    done <span style="color:#f92672">=</span> False
    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
        <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform() <span style="color:#f92672">&lt;</span> epsilon:
            action <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>sample()
        <span style="color:#66d9ef">else</span>:
            action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(Q_table[state])
        next_state, reward, done, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
        
        target <span style="color:#f92672">=</span> reward <span style="color:#f92672">+</span> gamma<span style="color:#f92672">*</span>Q_table[next_state, np<span style="color:#f92672">.</span>argmax(Q_table[next_state])] 
        delta <span style="color:#f92672">=</span> target <span style="color:#f92672">-</span> Q_table[state][action]
        Q_table[state][action] <span style="color:#f92672">+=</span> alpha <span style="color:#f92672">*</span> delta
        state <span style="color:#f92672">=</span> next_state</code></pre></div>
<pre><code>100%|██████████| 5000/5000 [00:03&lt;00:00, 1653.97it/s]
</code></pre>

<h2 id="해결-solution">해결 (Solution)</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
done <span style="color:#f92672">=</span> False

<span style="color:#75715e"># Initial world display</span>
world <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>render(mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ansi&#39;</span>)
display(Pretty(world))
sleep(<span style="color:#ae81ff">0.5</span>)

<span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
    action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(Q_table[state]) <span style="color:#75715e"># Optimal Policy</span>
    state, reward, done, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
    
    <span style="color:#75715e"># updated world display</span>
    world <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>render(mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ansi&#39;</span>)
    clear_output(wait<span style="color:#f92672">=</span>True)
    display(Pretty(world))
    sleep(<span style="color:#ae81ff">0.5</span>)
    
    <span style="color:#66d9ef">if</span> done <span style="color:#f92672">and</span> state <span style="color:#f92672">==</span> <span style="color:#ae81ff">63</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> 🎉👍 성공! 🍺🥇&#39;</span>)
            </code></pre></div>
<pre><code>  (Right)
SFFFFFFF
FFFFFFFF
FFFHFFFF
FFFFFHFF
FFFHFFFF
FHHFFFHF
FHFFHFHF
FFFHFFF[41mG[0m




 🎉👍 성공! 🍺🥇
</code></pre>
		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/environment/" rel="tag">Environment</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/agent/" rel="tag">Agent</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/state/" rel="tag">State</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/action/" rel="tag">Action</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/reward/" rel="tag">Reward</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/policy/" rel="tag">Policy</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/q-value/" rel="tag">Q-value</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/%F0%9D%9C%80-greedy/" rel="tag">𝜀-greedy</a></li>
	</ul>
</div>
		</footer>
	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/post/sarsa_learning/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">SARSA Learning</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/post/deep_q_network/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">Deep Q-Networks (DQN)</p></a>
	</div>
</nav>
<script src="https://utteranc.es/client.js"
        repo="skettee/blog-comments"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2019 skettee.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>