<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>SARSA Learning - 막돼먹은 엔지니어의 머신런닝 개발</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<meta property="og:title" content="SARSA Learning" />
<meta property="og:description" content="강화 학습(Reinforcement Learning)에서 사용하는 SARSA에 대해서 알아 보고 Gym에서 제공하는 문제를 해결하기 위한 알고리듬을 만들어 보자." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/sarsa_learning/" />
<meta property="article:published_time" content="2019-09-25T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-09-25T00:00:00+00:00" />

	
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="/" title="막돼먹은 엔지니어의 머신런닝 개발" rel="home">
				<div class="logo__title">막돼먹은 엔지니어의 머신런닝 개발</div>
				<div class="logo__tagline">딥러닝 모델링</div>
			</a>
		</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">SARSA Learning</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2019-09-25T00:00:00">2019-09-25</time>
</div>

<div class="meta__item-categories meta__item">
	<svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg>
	<span class="meta__text"><a class="meta__link" href="/categories/reinforcement-learning" rel="category">Reinforcement Learning</a>, <a class="meta__link" href="/categories/temporal-difference-learning" rel="category">Temporal-Difference Learning</a></span>
</div>
</div>
		</header>
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#문제-분석-problem-anaysis">문제 분석 (Problem Anaysis)</a></li>
<li><a href="#환경-environment">환경 (Environment)</a>
<ul>
<li><a href="#액션-action">액션 (Action)</a></li>
<li><a href="#상태-state">상태 (State)</a></li>
<li><a href="#보상-reward">보상 (Reward)</a></li>
</ul></li>
<li><a href="#에이전트-agent">에이전트 (Agent)</a>
<ul>
<li><a href="#정책-policy">정책 (Policy)</a></li>
<li><a href="#상태-가치-state-value">상태 가치 (State Value)</a></li>
<li><a href="#q-value-state-action-value">Q-value (State Action Value)</a></li>
<li><a href="#최적-가치-optimal-value">최적 가치 (Optimal Value)</a></li>
<li><a href="#최적-정책-optimal-policy">최적 정책 (Optimal Policy)</a></li>
<li><a href="#살사-sarsa">살사 (SARSA)</a></li>
<li><a href="#𝜀-greedy">𝜀-greedy</a></li>
</ul></li>
<li><a href="#학습-learning">학습 (Learning)</a></li>
<li><a href="#해결-solution">해결 (Solution)</a></li>
</ul></li>
</ul>
</nav>
	</div>
</div>
<div class="content post__content clearfix">
			<p>강화 학습(Reinforcement Learning)에서 사용하는 SARSA에 대해서 알아 보고 Gym에서 제공하는 문제를 해결하기 위한 알고리듬을 만들어 보자.</p>

<p>실제로 돌려 보고 싶으면 구글 코랩으로 ~</p>

<p><a href="https://colab.research.google.com/github/skettee/notebooks/blob/master/sarsa_learning.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></p>

<p>👤 상사</p>

<blockquote>
<p>몬테 카를로와 같이 한 판이 끝날때 까지 기다리지 않고<br />
한 칸 앞으로만 가도 학습이 되는게 있다고 하네?<br />
아래 체육관(Gym)에 가서<br />
&lsquo;얼음 호수8X8&rsquo; 문제를 그걸로 풀어 보게</p>

<p><a href="https://gym.openai.com/envs/FrozenLake8x8-v0/">https://gym.openai.com/envs/FrozenLake8x8-v0/</a></p>
</blockquote>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>또 다른 알고리듬이 등장 하겠군&hellip;<br />
배움에는 끝이 없구냥&hellip;</p>
</blockquote>

<h2 id="문제-분석-problem-anaysis">문제 분석 (Problem Anaysis)</h2>

<p>일단 Gym을 돌려 보자!</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> gym
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> time <span style="color:#f92672">import</span> sleep
<span style="color:#f92672">from</span> IPython.display <span style="color:#f92672">import</span> display, clear_output, Pretty</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#</span>
<span style="color:#75715e"># Environment</span>
<span style="color:#75715e">#</span>
env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#39;FrozenLake8x8-v0&#39;</span>, is_slippery<span style="color:#f92672">=</span>False) <span style="color:#75715e"># 얼음위에서 미끄러지지 않도록 설정</span>
state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()

<span style="color:#75715e"># Initial world display</span>
world <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>render(mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ansi&#39;</span>)
display(Pretty(world))
sleep(<span style="color:#ae81ff">0.5</span>)

<span style="color:#75715e">#</span>
<span style="color:#75715e"># Agent</span>
<span style="color:#75715e">#</span>
<span style="color:#66d9ef">for</span> step <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">100</span>):
    action <span style="color:#f92672">=</span>env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>sample()
    next_state, reward, done, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)    
    state <span style="color:#f92672">=</span> next_state
    
    <span style="color:#75715e"># updated world display</span>
    world <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>render(mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ansi&#39;</span>)
    clear_output(wait<span style="color:#f92672">=</span>True)
    display(Pretty(world))
    sleep(<span style="color:#ae81ff">0.5</span>)
    
    <span style="color:#66d9ef">if</span> done: <span style="color:#75715e"># an episode finished</span>
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Episode finished after {} timesteps&#34;</span><span style="color:#f92672">.</span>format(step<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>))
        <span style="color:#66d9ef">break</span></code></pre></div>
<pre><code>  (Down)
SFFFFFFF
FFFFFFFF
FFF[41mH[0mFFFF
FFFFFHFF
FFFHFFFF
FHHFFFHF
FHFFHFHF
FFFHFFFG



Episode finished after 24 timesteps
</code></pre>

<h2 id="환경-environment">환경 (Environment)</h2>

<p>&lsquo;얼음호수8X8&rsquo; 세계의 환경은 64개의 상태(State)와 4개의 액션(Action)으로 구성 되어 있다.</p>

<h3 id="액션-action">액션 (Action)</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">env<span style="color:#f92672">.</span>action_space</code></pre></div>
<pre><code>Discrete(4)
</code></pre>

<p>&lsquo;얼음호수8X8&rsquo; 세계에서는 4개의 액션이 존재한다. 그리고 각각의 액션이 번호로 지정 되어 있다.</p>

<p>\(A = \{0, 1, 2, 3\}\)</p>

<table>
<thead>
<tr>
<th>Num</th>
<th>Action</th>
</tr>
</thead>

<tbody>
<tr>
<td>0</td>
<td>왼쪽으로 이동 (Move Left)</td>
</tr>

<tr>
<td>1</td>
<td>아래로 이동 (Move Down)</td>
</tr>

<tr>
<td>2</td>
<td>오른쪽으로 이동 (Move Right)</td>
</tr>

<tr>
<td>3</td>
<td>위로 이동 (Move Up)</td>
</tr>
</tbody>
</table>

<h3 id="상태-state">상태 (State)</h3>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">env<span style="color:#f92672">.</span>observation_space</code></pre></div>
<pre><code>Discrete(64)
</code></pre>

<p>&lsquo;얼음호수8X8&rsquo;의 상태(State) \(S\)는 다음과 같이 각 상태가 0과 63까지 번호로 지정되어 있다.</p>

<p>\(S = \{0, 1, \cdots , 63\}\)</p>

<p>\(\begin{vmatrix}
0 &amp; 1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 &amp; 6 &amp; 7 \\
8 &amp; 9 &amp; 10 &amp; 11 &amp; 12 &amp; 13 &amp; 14 &amp; 15 \\
16 &amp; 17 &amp; 18 &amp; 19 &amp; 20 &amp; 21 &amp; 22 &amp; 23 \\
24 &amp; 25 &amp; 26 &amp; 27 &amp; 28 &amp; 29 &amp; 30 &amp; 31 \\
32 &amp; 33 &amp; 34 &amp; 35 &amp; 36 &amp; 37 &amp; 38 &amp; 39 \\
40 &amp; 41 &amp; 42 &amp; 43 &amp; 44 &amp; 45 &amp; 46 &amp; 47 \\
48 &amp; 49 &amp; 50 &amp; 51 &amp; 52 &amp; 53 &amp; 54 &amp; 55 \\
56 &amp; 57 &amp; 58 &amp; 59 &amp; 60 &amp; 61 &amp; 62 &amp; 63
\end{vmatrix}\)</p>

<p>그리고 각 상태마다 액션(action), 확률(probability), 다음 상태(next state), 보상(reward), 종료(done)가 <code>{action: [(probability, nextstate, reward, done)]}</code> 형식으로 정의 되어 있다.</p>

<p>에피소드가 종료되는 상태를 기록해 놓자</p>

<p>\(S_{\text{terminate}} = \{19, 29, 35, 41, 42, 49, 52, 54, 59, 63\}\)</p>

<h3 id="보상-reward">보상 (Reward)</h3>

<p>55번의 상태를 까보자</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">env<span style="color:#f92672">.</span>P[<span style="color:#ae81ff">55</span>]</code></pre></div>
<pre><code>{0: [(1.0, 54, 0.0, True)],
 1: [(1.0, 63, 1.0, True)],
 2: [(1.0, 55, 0.0, False)],
 3: [(1.0, 47, 0.0, False)]}
</code></pre>

<p>63번 상태로 이동하면 \(R = 1.0\) 을 얻고 에피소드가 끝이난다.</p>

<h2 id="에이전트-agent">에이전트 (Agent)</h2>

<p>에이전트는 환경에서 부터 주어진 상태(\(S\)), 액션(\(A\)), 보상(\(R\))을 가지고 가치(Q-value)를 계산하고 정책(Policy)을 수립해서 최적의 액션(\(\pi_*(a|s)\))을 수행해야 한다.</p>

<h3 id="정책-policy">정책 (Policy)</h3>

<p>에이전트가 주어진 상태에서 액션을 선택하는 확률이다.</p>

<p>\(\pi (a|s) = \mathbb P[A_t = a | S_t = s]\)</p>

<h3 id="상태-가치-state-value">상태 가치 (State Value)</h3>

<p>상태 가치는 상태 s 에서 기대되는 미래 보상의 합이다. 여기서 \(G_t\)는 벨만 방정식(Bellman Equation)에 의해서 보상 \(R\) 과 다음 스텝의 상태 가치 \(v(s&rsquo;)\) 로 분해 할 수 있다.</p>

<p>\(\begin{align}
v_{\pi}(s) &amp; = \mathbb E_{\pi} [G_t | S_t = s] \\
&amp; = \mathbb E_{\pi} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots | S_t = s] \\
&amp; = \mathbb E_{\pi} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \cdots) | S_t = s] \\
&amp; = \mathbb E_{\pi} [R_{t+1} + \gamma G_{t+1} | S_t = s] \\
&amp; = \mathbb E_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]
\end{align}\)</p>

<h3 id="q-value-state-action-value">Q-value (State Action Value)</h3>

<p>Q-value는 상태 s 에서 액션 a 를 수행할 경우에 기대되는 미래 보상의 합이다.  여기서 \(G_t\)는 벨만 방정식(Bellman Equation)에 의해서 보상 \(R\) 과 다음 스텝의 Q-value \(q(s&rsquo;, a&rsquo;)\) 로 분해 할 수 있다.</p>

<p>\(\begin{align}
q_{\pi}(s,a) &amp; = \mathbb E_{\pi} [G_t | S_t = s, A_t = a] \\
&amp; = \mathbb E_{\pi} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots | S_t = s, A_t = a] \\
&amp; = \mathbb E_{\pi} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \cdots) | S_t = s, A_t = a] \\
&amp; = \mathbb E_{\pi} [R_{t+1} + \gamma G_{t+1} | S_t = s, A_t = a] \\
&amp;= \mathbb E_{\pi} [R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]
\end{align}\)</p>

<h3 id="최적-가치-optimal-value">최적 가치 (Optimal Value)</h3>

<p>상태 s 에서 가장 큰 Q-value이다.</p>

<p>\(q_*(s,a) = \max_{\pi} q_{\pi}(s,a)\)</p>

<h3 id="최적-정책-optimal-policy">최적 정책 (Optimal Policy)</h3>

<p>\(q_*(s,a)\)가 결정 되면 \(\pi (a|s) = 1\) 이 된다. 즉 상태 s일때는 100% 액션 a를 수행 한다.</p>

<p>\(\pi_*(a|s) = \begin{cases}
1 &amp; \text{if } a= \text{argmax}_{a \in A} q_\star(s,a) \\
0 &amp; \text{otherwise}
\end{cases}\)</p>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>에이전트는<br />
각 상태(state)의 액션(action)마다<br />
Q-value를 계산한다.</p>

<p>몬테 카를로 방법은<br />
에피소드가 끝나고 나서 \(G_t\)를 이용해서 Q-value를 계산했다.</p>

<p>그런데<br />
\(G_t\) 대신에 \(R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1})\)을 이용하면<br />
한 스텝만 이동해도 Q-value를 계산할 수 있는<br />
신묘한 알고리듬이 된다.<br />
그것은 바로&hellip;</p>
</blockquote>

<h3 id="살사-sarsa">살사 (SARSA)</h3>

<p>State-Action-Reward-State-Action의 줄임말로서 미래 보상의 합 \(G_t\) 대신에 보상값 \(R\) 과 다음 스텝의 Q-value를 이용해서 Q-value를 최적화 하는 알고리듬이다.</p>

<p>\(\mathsf {Q(S, A) \leftarrow Q(S, A) + \alpha \left( R + \gamma Q(S&rsquo;, A&rsquo;) - Q(S, A) \right)}\)</p>

<p>\(\alpha\) : learning rate<br />
\(\gamma\) : 디스카운트 (discount factor)</p>

<h3 id="𝜀-greedy">𝜀-greedy</h3>

<p>다음 스텝의 액션을 선택하는 경우에 욕심쟁이(greedy)처럼 최고의 Q-value의 액션만을 선택하면 오류가 발생할 수 있다. 따라서 적당히 램덤하게 액션을 선택하는 경우를 추가한 것이 𝜀-greedy이다.</p>

<p>\(\pi \leftarrow \epsilon \text{-greedy(Q)}\)</p>

<p>𝜀의 확률로 랜덤 액션을 수행하고 (1-𝜀)의 확률로 Q-value가 가장 큰 액션을 수행한다.</p>

<h2 id="학습-learning">학습 (Learning)</h2>

<p>살사를 이용해서 최적의 Q-value를 찾아보자!</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> tqdm <span style="color:#f92672">import</span> tqdm

num_state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>observation_space<span style="color:#f92672">.</span>n
num_action <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>n
num_episode <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span>

<span style="color:#75715e"># Initialize Q_table </span>
Q_table <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform(low<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>, high<span style="color:#f92672">=</span><span style="color:#ae81ff">0.00000001</span>, size<span style="color:#f92672">=</span>(num_state, num_action))
<span style="color:#75715e"># Zero for terminate states</span>
<span style="color:#66d9ef">for</span> s <span style="color:#f92672">in</span> [<span style="color:#ae81ff">19</span>, <span style="color:#ae81ff">29</span>, <span style="color:#ae81ff">35</span>, <span style="color:#ae81ff">41</span>, <span style="color:#ae81ff">42</span>, <span style="color:#ae81ff">49</span>, <span style="color:#ae81ff">52</span>, <span style="color:#ae81ff">54</span>, <span style="color:#ae81ff">59</span>, <span style="color:#ae81ff">63</span>]:
    Q_table[s] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>

<span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> tqdm(range(num_episode)):
    state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
    done <span style="color:#f92672">=</span> False
    <span style="color:#75715e"># Hyper parameter</span>
    epsilon <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.3</span>
    alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
    gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.9</span>
    
    <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform() <span style="color:#f92672">&lt;</span> epsilon:
        action <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>sample()
    <span style="color:#66d9ef">else</span>:
        action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(Q_table[state])
    
    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
        next_state, reward, done, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
        <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>uniform() <span style="color:#f92672">&lt;</span> epsilon:
            next_action <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>action_space<span style="color:#f92672">.</span>sample()
        <span style="color:#66d9ef">else</span>:
            next_action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(Q_table[next_state])
        
        Q_table[state][action] <span style="color:#f92672">+=</span> alpha <span style="color:#f92672">*</span> (reward <span style="color:#f92672">+</span> gamma<span style="color:#f92672">*</span>Q_table[next_state, next_action] \
                                           <span style="color:#f92672">-</span> Q_table[state][action])
        state, action <span style="color:#f92672">=</span> next_state, next_action</code></pre></div>
<pre><code>100%|██████████| 1000/1000 [00:00&lt;00:00, 2898.93it/s]
</code></pre>

<h2 id="해결-solution">해결 (Solution)</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
done <span style="color:#f92672">=</span> False

<span style="color:#75715e"># Initial world display</span>
world <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>render(mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ansi&#39;</span>)
display(Pretty(world))
sleep(<span style="color:#ae81ff">0.5</span>)

<span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
    action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(Q_table[state]) <span style="color:#75715e"># Optimal Policy</span>
    state, reward, done, info <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(action)
    
    <span style="color:#75715e"># updated world display</span>
    world <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>render(mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;ansi&#39;</span>)
    clear_output(wait<span style="color:#f92672">=</span>True)
    display(Pretty(world))
    sleep(<span style="color:#ae81ff">0.5</span>)
    
    <span style="color:#66d9ef">if</span> done <span style="color:#f92672">and</span> state <span style="color:#f92672">==</span> <span style="color:#ae81ff">63</span>:
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74"> 🎉👍 성공! 🍺🥇&#39;</span>)</code></pre></div>
<pre><code>  (Down)
SFFFFFFF
FFFFFFFF
FFFHFFFF
FFFFFHFF
FFFHFFFF
FHHFFFHF
FHFFHFHF
FFFHFFF[41mG[0m




 🎉👍 성공! 🍺🥇
</code></pre>
		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/environment/" rel="tag">Environment</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/agent/" rel="tag">Agent</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/state/" rel="tag">State</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/action/" rel="tag">Action</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/reward/" rel="tag">Reward</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/policy/" rel="tag">Policy</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/q-value/" rel="tag">Q-value</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/sarsa/" rel="tag">SARSA</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/%F0%9D%9C%80-greedy/" rel="tag">𝜀-greedy</a></li>
	</ul>
</div>
		</footer>
	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/post/monte_carlo_learning/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">Monte Carlo Learning</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/post/q_learning/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">Q-Learning</p></a>
	</div>
</nav>
<script src="https://utteranc.es/client.js"
        repo="skettee/blog-comments"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2019 skettee.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>