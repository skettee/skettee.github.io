<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>로지스틱 회귀 (Logistic Regression) - 막돼먹은 엔지니어의 머신런닝 개발</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<meta property="og:title" content="로지스틱 회귀 (Logistic Regression)" />
<meta property="og:description" content="딥러닝의 세계로 들어가기 위해 알아야 하는 두번째 모델인 로지스틱 회귀(Logistic Regression)에 대해 알아보고 keras를 이용해서 모델링을 해보자!" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/logistic_regression/" />


	
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/agate.min.css">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="shortcut icon" href="/favicon.ico">
	
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="/" title="막돼먹은 엔지니어의 머신런닝 개발" rel="home">
				<div class="logo__title">막돼먹은 엔지니어의 머신런닝 개발</div>
				<div class="logo__tagline">딥러닝 모델링</div>
			</a>
		</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">로지스틱 회귀 (Logistic Regression)</h1>
			<div class="post__meta meta">
<div class="meta__item-categories meta__item">
	<svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg>
	<span class="meta__text"><a class="meta__link" href="/categories/deep-learning" rel="category">Deep Learning</a>, <a class="meta__link" href="/categories/logistic-regression" rel="category">Logistic Regression</a></span>
</div>
</div>
		</header>
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#문제-problem">문제 (Problem)</a></li>
<li><a href="#데이터-분석-data-analysis">데이터 분석 (Data Analysis)</a></li>
<li><a href="#데이터-변환-data-transformation">데이터 변환 (Data Transformation)</a></li>
<li><a href="#모델링-modeling">모델링 (Modeling)</a>
<ul>
<li><a href="#시그모이드-함수-sigmoid-function">시그모이드 함수 (Sigmoid function)</a></li>
<li><a href="#분류-classification-를-위한-손실-함수-cross-entropy-loss">분류(Classification)를 위한 손실 함수: Cross-entropy Loss</a></li>
<li><a href="#rmsprop">RMSProp</a></li>
<li><a href="#정리">정리</a></li>
</ul></li>
<li><a href="#텐서플로우-tensorflow-로-모델링-modeling">텐서플로우(Tensorflow)로 모델링(Modeling)</a>
<ul>
<li><a href="#정규화-normalization">정규화 (Normalization)</a></li>
<li><a href="#keras를-가지고-모델링-modeling-하기">Keras를 가지고 모델링(Modeling)하기</a></li>
<li><a href="#손실값의-변화를-그래프로-확인">손실값의 변화를 그래프로 확인</a></li>
<li><a href="#w-와-b-값을-확인">\(w\)와 \(b\)값을 확인</a></li>
<li><a href="#그래프로-확인">그래프로 확인</a></li>
</ul></li>
<li><a href="#해결-solution">해결 (Solution)</a></li>
</ul></li>
</ul>
</nav>
	</div>
</div>
<div class="content post__content clearfix">
			<p>딥러닝의 세계로 들어가기 위해 알아야 하는 두번째 모델인 로지스틱 회귀(Logistic Regression)에 대해 알아보고 keras를 이용해서 모델링을 해보자!</p>

<p>실제로 돌려 보고 싶으면 구글 코랩으로 ~</p>

<p><a href="https://colab.research.google.com/github/skettee/notebooks/blob/master/logistic_regression.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></p>

<h2 id="문제-problem">문제 (Problem)</h2>

<p>💰 고객</p>

<blockquote>
<p>덕분에 &lsquo;몸짱반&rsquo;에 들어 갔어요. 고마워요!</p>

<p>학교에 아주 인기가 많은 여학생이 있어요~<br />
그런데 이 여학생과 카톡 친구 맺기가 어려워요.<br />
아마 키크기를 기준으로 친추를 하는거 같아요.<br />
이 여학생이 친추한 사람의 키와 거절한 사람의 키 데이터를 가지고<br />
그 여학생이 나를 친추할지 거절할지 예측하는 프로그램을 만들어 주세요.</p>

<p>데이터는 아래에 있어요.</p>
</blockquote>

<pre><code class="language-python">height_data = [150.0, 150.8, 151.6, 152.4, 153.2, 154.0, 154.8, 155.7, 156.5, 157.3, 158.1, 158.9, 159.7, 160.6, 161.4, 162.2, 163.0, 163.8, 164.6, 165.5, 166.3, 167.1, 167.9, 168.7, 169.5, 170.4, 171.2, 172.0, 172.8, 173.6, 174.4, 175.3, 176.1, 176.9, 177.7, 178.5, 179.3, 180.2, 181.0, 181.8, 182.6, 183.4, 184.2, 185.1, 185.9, 186.7, 187.5, 188.3, 189.1, 190.0]
chinchu_data = ['no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes']
</code></pre>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>데이터 분석이 크게 의미가 있을 것 같지가 않고&hellip;<br />
일단 친추를 요청하면 결과를 쉽게 알 수 있을것 같은데&hellip;</p>
</blockquote>

<p>💰💰 고객</p>

<blockquote>
<p>더블!</p>
</blockquote>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>지금 바로 분석 들어갑니다~</p>
</blockquote>

<h2 id="데이터-분석-data-analysis">데이터 분석 (Data Analysis)</h2>

<p>데이터가 어떤 모양인지 확인해 보자.</p>

<pre><code class="language-python">%matplotlib inline

import matplotlib.pyplot as plt

plt.scatter(height_data, chinchu_data)
plt.xlabel('height (cm)')
plt.ylabel('chinchu (yes or no)')
plt.show()
</code></pre>

<p><img src="output_6_0.png" alt="png" /></p>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>아&hellip;<br />
앞에서 배운 &lsquo;선형 회귀&rsquo;로는<br />
답이 나올 것 같지 않다&hellip;</p>
</blockquote>

<h2 id="데이터-변환-data-transformation">데이터 변환 (Data Transformation)</h2>

<p>키와 친추 데이터를 각각 매트릭스로 변환하자<br />
 - 열의 크기는 데이터의 개수<br />
 - 컬럼의 크기는 측정한 항목의 개수</p>

<p>키 값을 입력하면 &lsquo;친추&rsquo;가능 여부(yes or no)를 예측해야 한다. 키 데이터를 입력 x라고 하고 친추 가능 여부를 출력 y라고 하자</p>

<ul>
<li>키 데이터는 50개의 &lsquo;키&rsquo;를 측정한 데이터가 있으므로 50X1 매트릭스<br /></li>
<li>친추 데이터는 50개의 &lsquo;친추 여부&rsquo;를 측정한 데이터가 있으므로 50X1 매트릭스이다.<br /></li>
</ul>

<p>손실 함수(Loss function)를 계산하기 위해서는 &lsquo;yes&rsquo;, &lsquo;no&rsquo;를 계산이 가능한 수로 표시해야 한다.<br />
여기에서는 &lsquo;yes&rsquo;와 &lsquo;no&rsquo; 두가지의 결과만 있으니까 &lsquo;yes&rsquo;를 1로, &lsquo;no&rsquo;를 0으로 변환한다.</p>

<pre><code class="language-python">def transform_y(y):
    if y == 'yes':
        return 1
    else:
        return 0

import numpy as np

x = np.array(height_data).reshape(len(height_data), 1)
y = np.array([transform_y(i) for i in chinchu_data ]).reshape(len(chinchu_data), 1)

plt.scatter(x, y)
plt.xlabel('height (cm)')
plt.ylabel('chinchu')
plt.show()
</code></pre>

<p><img src="output_9_0.png" alt="png" /></p>

<h2 id="모델링-modeling">모델링 (Modeling)</h2>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>딱봐도 선형 회귀(Linear Regression)모델링은 답이 아니다&hellip;<br />
\(y=wx+b\) 및 \(J(w,b)\), 그리고 경사 하강법(Gradient Descent)을 이용하면서<br />
위와 비슷한 그래프가 나올 수 있는 모델링이<br />
과연 존재할까?</p>
</blockquote>

<p><strong>계단 함수 (step function)</strong></p>

<p>위의 데이터 분포와 가장 유사한 모양을 가지는 계단 함수를 생각해 보자.</p>

<p>\(y=s_{c}(x)\\
\\
s_{c}(x) =
\begin{cases}
0, &amp; x &lt; c \\
1, &amp; x \ge c
\end{cases}\)</p>

<pre><code class="language-python">plt.step(x, y)
plt.show()
</code></pre>

<p><img src="output_12_0.png" alt="png" /></p>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>오! 뭔가 그럴싸한데&hellip;<br />
\(z=wx+b\)로 놓고<br />
\(\hat{y}=s_{c}(z)\)로 모델링해서<br />
\(J(w,b)\) 함수를 그려보자</p>
</blockquote>

<pre><code class="language-python">import numpy as np
from sklearn.metrics import mean_squared_error
from mpl_toolkits import mplot3d

# w,d의 범위를 결정한다.
w = np.arange(-10, 10, 0.1)
d = np.arange(160, 180, 1)
j_array = []

# (20, 200) 매트릭스로 변환한다.
W, D = np.meshgrid(w, d)

# w, b를 하나씩 대응한다.
for we, de in zip(np.ravel(W), np.ravel(D)):
    z_hat = np.multiply(we, x)
    y_list = []
    for ze in z_hat:
        if ze &lt; de:
            y_list.append(0)
        else:
            y_list.append(1)
    y_hat = np.array(y_list)
    # Cost function
    mse = mean_squared_error(y_hat, y) / 2.0
    j_array.append(mse)

# 손실(Loss)을 구하고 (20, 200) 매트릭스로 변환한다.
J = np.array(j_array).reshape(W.shape)

# 서피스 그래프를 그린다.
fig = plt.figure()
ax = plt.axes(projection=&quot;3d&quot;)

ax.plot_surface(W, D, J, color='b', alpha=0.5)
ax.set_xlabel('w')
ax.set_ylabel('d')
ax.set_zlabel('J')
plt.show()
</code></pre>

<p><img src="output_14_0.png" alt="png" /></p>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>내 이럴줄 알았다&hellip;<br />
결론적으로 계단 함수(Step function)는 사용할 수 없다!<br />
왜냐하면&hellip;</p>

<p>\(J(w,b)\)가 <strong>미분 불가능</strong> 하기 때문이다!</p>

<p>미분 가능하면서도 계단 함수와 비슷한 함수를 찾아야 한다&hellip;</p>

<p>다행히<br />
<strong>S라인</strong>의 멋진 함수가 있다!</p>

<p>그것은 바로~</p>
</blockquote>

<h3 id="시그모이드-함수-sigmoid-function">시그모이드 함수 (Sigmoid function)</h3>

<p>엔지니어들은 미분이 잘 되고 \(x\)에 어떤 값을 넣어도 0이나 1의 값에 가까운 값을 가지는 함수를 만들어 냈다.<br />
이것이 시그모이드 함수다.</p>

<p>\(\sigma(x) = {1 \over {1 + e^{-x}}}\)</p>

<pre><code class="language-python">sigmoid = lambda x: 1.0 / (1.0 + np.exp(-x))

xx = np.linspace(-10,10,100)

plt.plot(xx, sigmoid(xx))
plt.show()
</code></pre>

<p><img src="output_17_0.png" alt="png" /></p>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>오! 멋진데&hellip;<br />
\(z=wx+b\)로 놓고<br />
\(\hat{y}=\sigma(z)\)로 모델링해서<br />
\(J(w,b)\) 함수를 그려보자</p>

<p>여기서 우리는 새로운 손실 함수(Loss function)를 사용할 것이다.</p>
</blockquote>

<h3 id="분류-classification-를-위한-손실-함수-cross-entropy-loss">분류(Classification)를 위한 손실 함수: Cross-entropy Loss</h3>

<p>하나의 데이터 세트(\(x^{(i)}, y^{(i)}\))를 사용해서 모델에서 얻은 값과 실제 값과의 차이(Loss)를 구하는 함수를 구해보자.<br />
여기서 \(x^{(i)}\)는 i번째 \(x\)값이고 \(y^{(i)}\)은 i번째 \(y\)값이다.</p>

<p>일단 \(w\)와 \(b\)는 임의의 값으로 놓자. 그리고 모델에 \(x^{(i)}\)을 넣고 계산한 결과 값 \({\hat y}^{(i)}\)과 실제 값 \(y^{(i)}\)의 차이를 구한다. 로지스틱 모델링에서는 크로스 엔트로피 손실(Cross-entropy loss)을 사용한다.</p>

<p>\(L({\hat y}^{(i)}, y^{(i)})=-\bigl (y^{(i)}log({\hat y}^{(i)}) + (1-y^{(i)})log(1-{\hat y}^{(i)})\bigr)\)</p>

<p>모든 데이터(m개의 데이터 세트)로 부터 얻은 것을 평균 한것이 손실 함수(Loss function)이다. 손실 함수는 \(w\)와 \(b\)의 함수로 나타낼 수 있다.</p>

<p>\({\large J}(w, b) = {1\over m}\sum_{i=1}^m L({\hat y}^{(i)}, y^{(i)}) \\
\\
\hspace{2.9em}= -{1\over {m}}\sum_{i=1}^m [(y^{(i)}log({\hat y}^{(i)})) + (1-y^{(i)})log(1-{\hat y}^{(i)})]\)</p>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>이제부터<br />
선형 회귀(Linear regression) 모델에서 사용된 손실 함수인<br />
평균 제곱 오차 (mean squared error)를 사용하지 않고<br />
고양이냐 개냐 토끼냐, Yes or No 등의 분류(Classification) 문제를 위한 손실 함수로서<br />
아래의 손실 함수를 사용한다.</p>

<p>\({\large J}(w, b) = -{1\over {m}}\sum_{i=1}^m [(y^{(i)}log({\hat y}^{(i)})) + (1-y^{(i)})log(1-{\hat y}^{(i)})]\)</p>

<p>엔지니어를 갈아서 만든 것이니<br />
우리는 사용하기만 하면 된다</p>
</blockquote>

<p><strong>손실 함수 (Loss function) 시각화</strong></p>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>우선 손실 함수(Loss function)가 어떻게 생겨 먹었는지 살펴 보자.<br />
x축을 \(w\)로 놓고, y축을 \(b\)로 놓고, z축을 손실 함수 \({\large J}(w, b)\)로
그래프를 그려 보면<br />
어떻게 최소값을 찾을지 감이 올 것 같다.</p>
</blockquote>

<pre><code class="language-python">from sklearn.metrics import log_loss

cross_entropy_loss = True

# W,b의 범위를 결정한다.
w = np.arange(20, 30, 0.1)
b = np.arange(-4595, -4585, 0.1)

j_loss = []

# 매트릭스로 변환한다.
W, B = np.meshgrid(w, b)

# w, b를 하나씩 대응한다.
for we, be in zip(np.ravel(W), np.ravel(B)):
    z = np.add(np.multiply(we, x), be)
    y_hat = sigmoid(z)
    # Loss function
    if cross_entropy_loss: 
        loss = log_loss(y, y_hat) # Log loss, aka logistic loss or cross-entropy loss.
        j_loss.append(loss)
    else:
        loss = mean_squared_error(y_hat, y) / 2.0 # Mean squred error
        j_loss.append(loss)

# 손실(Loss)을 구한다.
J = np.array(j_loss).reshape(W.shape)

# 서피스 그래프를 그린다.
fig = plt.figure()
ax = plt.axes(projection=&quot;3d&quot;)
ax.plot_surface(W, B, J, color='b', alpha=0.5)
ax.set_xlabel('w')
ax.set_ylabel('b')
ax.set_zlabel('J')
plt.show()

</code></pre>

<p><img src="output_21_0.png" alt="png" /></p>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>V 모양으로 구부러진 모양이다!<br />
\(w\)가 27근처에서 손실 함수가 최소값을 가지는 것을 볼 수 있다.</p>

<p>손실 함수가 최소가 되는 \(w\)와 \(b\)를 빠르게 찾기 위해서<br />
경사 하강법(Gradient Descent)보다 성능이 우수한<br />
<strong>RMSProp</strong> 을 사용한다.</p>
</blockquote>

<h3 id="rmsprop">RMSProp</h3>

<p>RMSProp의 원리는 급경사인 경우에는 보폭을 낮추어서 가장 아래인지를 세밀히 살피고, 완만한 경사인 경우에는 보폭을 넓혀서 빨리 지나가는 방식이다.<br />
이 방식은 매우 빠르게 손실 함수의 최소값을 찾을 수 있다.</p>

<p>\(dw = {\partial {J(w,b)}\over \partial w}\),<br />
\(db = {\partial{J(w,b)}\over \partial b}\)</p>

<p>REPEAT(epoch) {<br />
\(w:=w-\alpha {dw \over {\sqrt {S_{dw}} + \epsilon}}\)</p>

<p>\(b:=b-\alpha {db \over {\sqrt {S_{db}} + \epsilon}}\)<br />
}</p>

<p>\(S_{dw} = \rho S_{dw} + (1-\rho)dw^2\),<br />
\(S_{db} = \rho S_{db} + (1-\rho)db^2\)</p>

<p>\(\alpha=0.001\) : learining rate,<br />
\(\rho=0.9\) : discounting factor,<br />
\(\epsilon=1e-07\) : small value to avoid zero denominator</p>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>이것도 엔지니어를 갈아서 만든 것이니<br />
우리는 사용하기만 하면 된다</p>
</blockquote>

<h3 id="정리">정리</h3>

<p>로지스틱 모델(Logistic model)을 만드는 방법을 정리해 보자.</p>

<ol>
<li>\(z=wx+b\) 함수를 정의한다.<br /></li>
<li>\(a = \sigma(z)\) 함수를 정의한다. \(a\)를 <strong>활성 함수(activation function)</strong> 라고 한다.<br /></li>
<li>\(\hat{y} = a\) 이다.</li>
<li>손실 함수 (Loss function)를 정의한다. 여기서는 <strong>크로스-엔트로피 손실(cross-entropy loss)</strong> 를 사용한다.<br /></li>
<li>옵티마이저(Optimizer)를 선택한다. 여기서는 <strong>RMSProp</strong>을 사용한다.<br /></li>
<li>반복할 회수(epoch)를 결정한다.<br /></li>
<li>주어진 조건으로 모델을 최적화(fit) 시킨다.<br /></li>
</ol>

<h2 id="텐서플로우-tensorflow-로-모델링-modeling">텐서플로우(Tensorflow)로 모델링(Modeling)</h2>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>좋았어!</p>

<p><strong>케라스(Keras)</strong>를 이용해서 구현을 해보자!</p>
</blockquote>

<h3 id="정규화-normalization">정규화 (Normalization)</h3>

<p><strong>정규값 = (현재값 - 최소값) / (최대값-최소값)</strong> 으로 정규화 한다!</p>

<p>그래프를 보면,<br />
데이터의 모양은 그대로 유지하면서도 \(x\)축의 값이 0에서 1사이로 변환된 것을 볼 수 있다.</p>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>이제 정규화는<br />
선택이 아닌 <strong>필수</strong>!</p>
</blockquote>

<pre><code class="language-python">from sklearn import preprocessing

mm_scaler = preprocessing.MinMaxScaler()
X_train = mm_scaler.fit_transform(x)
Y_train = y

plt.scatter(X_train, Y_train)
plt.xlabel('scaled-height')
plt.ylabel('chinchu')
plt.show()
</code></pre>

<p><img src="output_27_0.png" alt="png" /></p>

<h3 id="keras를-가지고-모델링-modeling-하기">Keras를 가지고 모델링(Modeling)하기</h3>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>4줄로 모델링이 가능하다!</p>

<p>케라스 만만세!</p>
</blockquote>

<pre><code class="language-python">from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Activation

# 모델을 준비한다.
model = Sequential()

# 입력 변수의 개수가 1이고 출력 개수가 1인 y=sigmoid(wx+b)를 생성한다.
model.add(Dense(1, input_dim=1, activation='sigmoid'))

# Loss funtion과 Optimizer를 선택한다.
model.compile(loss='binary_crossentropy', optimizer='rmsprop') 

# epochs만큼 반복해서 손실값이 최저가 되도록 모델을 훈련한다.
hist = model.fit(X_train, Y_train, epochs=10000, batch_size=20, verbose=0) 
</code></pre>

<pre><code>WARNING: Logging before flag parsing goes to stderr.
W0901 10:08:24.463013 140371975219008 deprecation.py:506] From /home/dataman/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0901 10:08:24.482560 140371975219008 deprecation.py:323] From /home/dataman/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
</code></pre>

<h3 id="손실값의-변화를-그래프로-확인">손실값의 변화를 그래프로 확인</h3>

<pre><code class="language-python">plt.plot(hist.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
</code></pre>

<p><img src="output_31_0.png" alt="png" /></p>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>반복적으로 학습할 수록 손실(Loss)이 0에 가깝게 된다.<br />
나이스!</p>
</blockquote>

<h3 id="w-와-b-값을-확인">\(w\)와 \(b\)값을 확인</h3>

<pre><code class="language-python">w, b = model.get_weights()
w =  w[0][0]
b = b[0]
print('w: ', w)
print('b: ', b)
</code></pre>

<pre><code>w:  14.712823
b:  -7.5659995
</code></pre>

<h3 id="그래프로-확인">그래프로 확인</h3>

<pre><code class="language-python">x_scale = mm_scaler.transform(x)
plt.scatter(x_scale, y)
plt.plot(x_scale, sigmoid(w*np.array(x_scale)+b), 'r')
plt.xlabel('scaled-height')
plt.ylabel('chinchu')
plt.show()
</code></pre>

<p><img src="output_36_0.png" alt="png" /></p>

<h2 id="해결-solution">해결 (Solution)</h2>

<p>⚙️ 엔지니어</p>

<blockquote>
<p>고객님~ 원하시는 솔루션입니다.<br />
input_height에 원하시는 키를 입력하시면<br />
&lsquo;친추&rsquo;가 될 확률을 알려 줍니다.</p>
</blockquote>

<pre><code class="language-python">input_height = 178.0

input_x = mm_scaler.transform(np.array([input_height]).reshape(-1, 1))
predict = model.predict(input_x)

print('친추가 될 확률은 {:.1f}% 입니다.'.format(predict[0][0]*100))
</code></pre>

<pre><code>친추가 될 확률은 93.9% 입니다.
</code></pre>
		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/" rel="tag">딥러닝</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80/" rel="tag">로지스틱회귀</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C/" rel="tag">시그모이드</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/%ED%99%9C%EC%84%B1-%ED%95%A8%EC%88%98/" rel="tag">활성 함수</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/cross-entropy-loss/" rel="tag">Cross entropy loss</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/rmsprop/" rel="tag">RMSProp</a></li>
	</ul>
</div>
		</footer>
	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/post/linear_regression/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">선형 회귀 (Linear Regression)</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/post/neural_network/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">뉴럴 네트워크 (Neural Network)</p></a>
	</div>
</nav>
<script src="https://utteranc.es/client.js"
        repo="skettee/blog-comments"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2019 막돼먹은 엔지니어의 머신런닝 개발.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>