<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regression) - ë§‰ë¼ë¨¹ì€ ì—”ì§€ë‹ˆì–´ì˜ ë¨¸ì‹ ëŸ°ë‹ ê°œë°œ</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<meta property="og:title" content="ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regression)" />
<meta property="og:description" content="ë”¥ëŸ¬ë‹ì˜ ì„¸ê³„ë¡œ ë“¤ì–´ê°€ê¸° ìœ„í•´ ì•Œì•„ì•¼ í•˜ëŠ” ë‘ë²ˆì§¸ ëª¨ë¸ì¸ ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression)ì— ëŒ€í•´ ì•Œì•„ë³´ê³  kerasë¥¼ ì´ìš©í•´ì„œ ëª¨ë¸ë§ì„ í•´ë³´ì!" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/logistic_regression/" />


	
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/agate.min.css">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="shortcut icon" href="/favicon.ico">
	
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="/" title="ë§‰ë¼ë¨¹ì€ ì—”ì§€ë‹ˆì–´ì˜ ë¨¸ì‹ ëŸ°ë‹ ê°œë°œ" rel="home">
				<div class="logo__title">ë§‰ë¼ë¨¹ì€ ì—”ì§€ë‹ˆì–´ì˜ ë¨¸ì‹ ëŸ°ë‹ ê°œë°œ</div>
				<div class="logo__tagline">ë”¥ëŸ¬ë‹ ëª¨ë¸ë§</div>
			</a>
		</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ë¡œì§€ìŠ¤í‹± íšŒê·€ (Logistic Regression)</h1>
			<div class="post__meta meta">
<div class="meta__item-categories meta__item">
	<svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg>
	<span class="meta__text"><a class="meta__link" href="/categories/deep-learning" rel="category">Deep Learning</a>, <a class="meta__link" href="/categories/logistic-regression" rel="category">Logistic Regression</a></span>
</div>
</div>
		</header>
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#ë¬¸ì œ-problem">ë¬¸ì œ (Problem)</a></li>
<li><a href="#ë°ì´í„°-ë¶„ì„-data-analysis">ë°ì´í„° ë¶„ì„ (Data Analysis)</a></li>
<li><a href="#ë°ì´í„°-ë³€í™˜-data-transformation">ë°ì´í„° ë³€í™˜ (Data Transformation)</a></li>
<li><a href="#ëª¨ë¸ë§-modeling">ëª¨ë¸ë§ (Modeling)</a>
<ul>
<li><a href="#ì‹œê·¸ëª¨ì´ë“œ-í•¨ìˆ˜-sigmoid-function">ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ (Sigmoid function)</a></li>
<li><a href="#ë¶„ë¥˜-classification-ë¥¼-ìœ„í•œ-ì†ì‹¤-í•¨ìˆ˜-cross-entropy-loss">ë¶„ë¥˜(Classification)ë¥¼ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜: Cross-entropy Loss</a></li>
<li><a href="#rmsprop">RMSProp</a></li>
<li><a href="#ì •ë¦¬">ì •ë¦¬</a></li>
</ul></li>
<li><a href="#í…ì„œí”Œë¡œìš°-tensorflow-ë¡œ-ëª¨ë¸ë§-modeling">í…ì„œí”Œë¡œìš°(Tensorflow)ë¡œ ëª¨ë¸ë§(Modeling)</a>
<ul>
<li><a href="#ì •ê·œí™”-normalization">ì •ê·œí™” (Normalization)</a></li>
<li><a href="#kerasë¥¼-ê°€ì§€ê³ -ëª¨ë¸ë§-modeling-í•˜ê¸°">Kerasë¥¼ ê°€ì§€ê³  ëª¨ë¸ë§(Modeling)í•˜ê¸°</a></li>
<li><a href="#ì†ì‹¤ê°’ì˜-ë³€í™”ë¥¼-ê·¸ë˜í”„ë¡œ-í™•ì¸">ì†ì‹¤ê°’ì˜ ë³€í™”ë¥¼ ê·¸ë˜í”„ë¡œ í™•ì¸</a></li>
<li><a href="#w-ì™€-b-ê°’ì„-í™•ì¸">\(w\)ì™€ \(b\)ê°’ì„ í™•ì¸</a></li>
<li><a href="#ê·¸ë˜í”„ë¡œ-í™•ì¸">ê·¸ë˜í”„ë¡œ í™•ì¸</a></li>
</ul></li>
<li><a href="#í•´ê²°-solution">í•´ê²° (Solution)</a></li>
</ul></li>
</ul>
</nav>
	</div>
</div>
<div class="content post__content clearfix">
			<p>ë”¥ëŸ¬ë‹ì˜ ì„¸ê³„ë¡œ ë“¤ì–´ê°€ê¸° ìœ„í•´ ì•Œì•„ì•¼ í•˜ëŠ” ë‘ë²ˆì§¸ ëª¨ë¸ì¸ ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression)ì— ëŒ€í•´ ì•Œì•„ë³´ê³  kerasë¥¼ ì´ìš©í•´ì„œ ëª¨ë¸ë§ì„ í•´ë³´ì!</p>

<p>ì‹¤ì œë¡œ ëŒë ¤ ë³´ê³  ì‹¶ìœ¼ë©´ êµ¬ê¸€ ì½”ë©ìœ¼ë¡œ ~</p>

<p><a href="https://colab.research.google.com/github/skettee/notebooks/blob/master/logistic_regression.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></p>

<h2 id="ë¬¸ì œ-problem">ë¬¸ì œ (Problem)</h2>

<p>ğŸ’° ê³ ê°</p>

<blockquote>
<p>ë•ë¶„ì— &lsquo;ëª¸ì§±ë°˜&rsquo;ì— ë“¤ì–´ ê°”ì–´ìš”. ê³ ë§ˆì›Œìš”!</p>

<p>í•™êµì— ì•„ì£¼ ì¸ê¸°ê°€ ë§ì€ ì—¬í•™ìƒì´ ìˆì–´ìš”~<br />
ê·¸ëŸ°ë° ì´ ì—¬í•™ìƒê³¼ ì¹´í†¡ ì¹œêµ¬ ë§ºê¸°ê°€ ì–´ë ¤ì›Œìš”.<br />
ì•„ë§ˆ í‚¤í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì¹œì¶”ë¥¼ í•˜ëŠ”ê±° ê°™ì•„ìš”.<br />
ì´ ì—¬í•™ìƒì´ ì¹œì¶”í•œ ì‚¬ëŒì˜ í‚¤ì™€ ê±°ì ˆí•œ ì‚¬ëŒì˜ í‚¤ ë°ì´í„°ë¥¼ ê°€ì§€ê³ <br />
ê·¸ ì—¬í•™ìƒì´ ë‚˜ë¥¼ ì¹œì¶”í• ì§€ ê±°ì ˆí• ì§€ ì˜ˆì¸¡í•˜ëŠ” í”„ë¡œê·¸ë¨ì„ ë§Œë“¤ì–´ ì£¼ì„¸ìš”.</p>

<p>ë°ì´í„°ëŠ” ì•„ë˜ì— ìˆì–´ìš”.</p>
</blockquote>

<pre><code class="language-python">height_data = [150.0, 150.8, 151.6, 152.4, 153.2, 154.0, 154.8, 155.7, 156.5, 157.3, 158.1, 158.9, 159.7, 160.6, 161.4, 162.2, 163.0, 163.8, 164.6, 165.5, 166.3, 167.1, 167.9, 168.7, 169.5, 170.4, 171.2, 172.0, 172.8, 173.6, 174.4, 175.3, 176.1, 176.9, 177.7, 178.5, 179.3, 180.2, 181.0, 181.8, 182.6, 183.4, 184.2, 185.1, 185.9, 186.7, 187.5, 188.3, 189.1, 190.0]
chinchu_data = ['no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes', 'yes']
</code></pre>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ë°ì´í„° ë¶„ì„ì´ í¬ê²Œ ì˜ë¯¸ê°€ ìˆì„ ê²ƒ ê°™ì§€ê°€ ì•Šê³ &hellip;<br />
ì¼ë‹¨ ì¹œì¶”ë¥¼ ìš”ì²­í•˜ë©´ ê²°ê³¼ë¥¼ ì‰½ê²Œ ì•Œ ìˆ˜ ìˆì„ê²ƒ ê°™ì€ë°&hellip;</p>
</blockquote>

<p>ğŸ’°ğŸ’° ê³ ê°</p>

<blockquote>
<p>ë”ë¸”!</p>
</blockquote>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ì§€ê¸ˆ ë°”ë¡œ ë¶„ì„ ë“¤ì–´ê°‘ë‹ˆë‹¤~</p>
</blockquote>

<h2 id="ë°ì´í„°-ë¶„ì„-data-analysis">ë°ì´í„° ë¶„ì„ (Data Analysis)</h2>

<p>ë°ì´í„°ê°€ ì–´ë–¤ ëª¨ì–‘ì¸ì§€ í™•ì¸í•´ ë³´ì.</p>

<pre><code class="language-python">%matplotlib inline

import matplotlib.pyplot as plt

plt.scatter(height_data, chinchu_data)
plt.xlabel('height (cm)')
plt.ylabel('chinchu (yes or no)')
plt.show()
</code></pre>

<p><img src="output_6_0.png" alt="png" /></p>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ì•„&hellip;<br />
ì•ì—ì„œ ë°°ìš´ &lsquo;ì„ í˜• íšŒê·€&rsquo;ë¡œëŠ”<br />
ë‹µì´ ë‚˜ì˜¬ ê²ƒ ê°™ì§€ ì•Šë‹¤&hellip;</p>
</blockquote>

<h2 id="ë°ì´í„°-ë³€í™˜-data-transformation">ë°ì´í„° ë³€í™˜ (Data Transformation)</h2>

<p>í‚¤ì™€ ì¹œì¶” ë°ì´í„°ë¥¼ ê°ê° ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³€í™˜í•˜ì<br />
 - ì—´ì˜ í¬ê¸°ëŠ” ë°ì´í„°ì˜ ê°œìˆ˜<br />
 - ì»¬ëŸ¼ì˜ í¬ê¸°ëŠ” ì¸¡ì •í•œ í•­ëª©ì˜ ê°œìˆ˜</p>

<p>í‚¤ ê°’ì„ ì…ë ¥í•˜ë©´ &lsquo;ì¹œì¶”&rsquo;ê°€ëŠ¥ ì—¬ë¶€(yes or no)ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•œë‹¤. í‚¤ ë°ì´í„°ë¥¼ ì…ë ¥ xë¼ê³  í•˜ê³  ì¹œì¶” ê°€ëŠ¥ ì—¬ë¶€ë¥¼ ì¶œë ¥ yë¼ê³  í•˜ì</p>

<ul>
<li>í‚¤ ë°ì´í„°ëŠ” 50ê°œì˜ &lsquo;í‚¤&rsquo;ë¥¼ ì¸¡ì •í•œ ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ 50X1 ë§¤íŠ¸ë¦­ìŠ¤<br /></li>
<li>ì¹œì¶” ë°ì´í„°ëŠ” 50ê°œì˜ &lsquo;ì¹œì¶” ì—¬ë¶€&rsquo;ë¥¼ ì¸¡ì •í•œ ë°ì´í„°ê°€ ìˆìœ¼ë¯€ë¡œ 50X1 ë§¤íŠ¸ë¦­ìŠ¤ì´ë‹¤.<br /></li>
</ul>

<p>ì†ì‹¤ í•¨ìˆ˜(Loss function)ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œëŠ” &lsquo;yes&rsquo;, &lsquo;no&rsquo;ë¥¼ ê³„ì‚°ì´ ê°€ëŠ¥í•œ ìˆ˜ë¡œ í‘œì‹œí•´ì•¼ í•œë‹¤.<br />
ì—¬ê¸°ì—ì„œëŠ” &lsquo;yes&rsquo;ì™€ &lsquo;no&rsquo; ë‘ê°€ì§€ì˜ ê²°ê³¼ë§Œ ìˆìœ¼ë‹ˆê¹Œ &lsquo;yes&rsquo;ë¥¼ 1ë¡œ, &lsquo;no&rsquo;ë¥¼ 0ìœ¼ë¡œ ë³€í™˜í•œë‹¤.</p>

<pre><code class="language-python">def transform_y(y):
    if y == 'yes':
        return 1
    else:
        return 0

import numpy as np

x = np.array(height_data).reshape(len(height_data), 1)
y = np.array([transform_y(i) for i in chinchu_data ]).reshape(len(chinchu_data), 1)

plt.scatter(x, y)
plt.xlabel('height (cm)')
plt.ylabel('chinchu')
plt.show()
</code></pre>

<p><img src="output_9_0.png" alt="png" /></p>

<h2 id="ëª¨ë¸ë§-modeling">ëª¨ë¸ë§ (Modeling)</h2>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ë”±ë´ë„ ì„ í˜• íšŒê·€(Linear Regression)ëª¨ë¸ë§ì€ ë‹µì´ ì•„ë‹ˆë‹¤&hellip;<br />
\(y=wx+b\) ë° \(J(w,b)\), ê·¸ë¦¬ê³  ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ì„ ì´ìš©í•˜ë©´ì„œ<br />
ìœ„ì™€ ë¹„ìŠ·í•œ ê·¸ë˜í”„ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë¸ë§ì´<br />
ê³¼ì—° ì¡´ì¬í• ê¹Œ?</p>
</blockquote>

<p><strong>ê³„ë‹¨ í•¨ìˆ˜ (step function)</strong></p>

<p>ìœ„ì˜ ë°ì´í„° ë¶„í¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ ëª¨ì–‘ì„ ê°€ì§€ëŠ” ê³„ë‹¨ í•¨ìˆ˜ë¥¼ ìƒê°í•´ ë³´ì.</p>

<p>\(y=s_{c}(x)\\
\\
s_{c}(x) =
\begin{cases}
0, &amp; x &lt; c \\
1, &amp; x \ge c
\end{cases}\)</p>

<pre><code class="language-python">plt.step(x, y)
plt.show()
</code></pre>

<p><img src="output_12_0.png" alt="png" /></p>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ì˜¤! ë­”ê°€ ê·¸ëŸ´ì‹¸í•œë°&hellip;<br />
\(z=wx+b\)ë¡œ ë†“ê³ <br />
\(\hat{y}=s_{c}(z)\)ë¡œ ëª¨ë¸ë§í•´ì„œ<br />
\(J(w,b)\) í•¨ìˆ˜ë¥¼ ê·¸ë ¤ë³´ì</p>
</blockquote>

<pre><code class="language-python">import numpy as np
from sklearn.metrics import mean_squared_error
from mpl_toolkits import mplot3d

# w,dì˜ ë²”ìœ„ë¥¼ ê²°ì •í•œë‹¤.
w = np.arange(-10, 10, 0.1)
d = np.arange(160, 180, 1)
j_array = []

# (20, 200) ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³€í™˜í•œë‹¤.
W, D = np.meshgrid(w, d)

# w, bë¥¼ í•˜ë‚˜ì”© ëŒ€ì‘í•œë‹¤.
for we, de in zip(np.ravel(W), np.ravel(D)):
    z_hat = np.multiply(we, x)
    y_list = []
    for ze in z_hat:
        if ze &lt; de:
            y_list.append(0)
        else:
            y_list.append(1)
    y_hat = np.array(y_list)
    # Cost function
    mse = mean_squared_error(y_hat, y) / 2.0
    j_array.append(mse)

# ì†ì‹¤(Loss)ì„ êµ¬í•˜ê³  (20, 200) ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³€í™˜í•œë‹¤.
J = np.array(j_array).reshape(W.shape)

# ì„œí”¼ìŠ¤ ê·¸ë˜í”„ë¥¼ ê·¸ë¦°ë‹¤.
fig = plt.figure()
ax = plt.axes(projection=&quot;3d&quot;)

ax.plot_surface(W, D, J, color='b', alpha=0.5)
ax.set_xlabel('w')
ax.set_ylabel('d')
ax.set_zlabel('J')
plt.show()
</code></pre>

<p><img src="output_14_0.png" alt="png" /></p>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ë‚´ ì´ëŸ´ì¤„ ì•Œì•˜ë‹¤&hellip;<br />
ê²°ë¡ ì ìœ¼ë¡œ ê³„ë‹¨ í•¨ìˆ˜(Step function)ëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤!<br />
ì™œëƒí•˜ë©´&hellip;</p>

<p>\(J(w,b)\)ê°€ <strong>ë¯¸ë¶„ ë¶ˆê°€ëŠ¥</strong> í•˜ê¸° ë•Œë¬¸ì´ë‹¤!</p>

<p>ë¯¸ë¶„ ê°€ëŠ¥í•˜ë©´ì„œë„ ê³„ë‹¨ í•¨ìˆ˜ì™€ ë¹„ìŠ·í•œ í•¨ìˆ˜ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤&hellip;</p>

<p>ë‹¤í–‰íˆ<br />
<strong>Së¼ì¸</strong>ì˜ ë©‹ì§„ í•¨ìˆ˜ê°€ ìˆë‹¤!</p>

<p>ê·¸ê²ƒì€ ë°”ë¡œ~</p>
</blockquote>

<h3 id="ì‹œê·¸ëª¨ì´ë“œ-í•¨ìˆ˜-sigmoid-function">ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ (Sigmoid function)</h3>

<p>ì—”ì§€ë‹ˆì–´ë“¤ì€ ë¯¸ë¶„ì´ ì˜ ë˜ê³  \(x\)ì— ì–´ë–¤ ê°’ì„ ë„£ì–´ë„ 0ì´ë‚˜ 1ì˜ ê°’ì— ê°€ê¹Œìš´ ê°’ì„ ê°€ì§€ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ëƒˆë‹¤.<br />
ì´ê²ƒì´ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë‹¤.</p>

<p>\(\sigma(x) = {1 \over {1 + e^{-x}}}\)</p>

<pre><code class="language-python">sigmoid = lambda x: 1.0 / (1.0 + np.exp(-x))

xx = np.linspace(-10,10,100)

plt.plot(xx, sigmoid(xx))
plt.show()
</code></pre>

<p><img src="output_17_0.png" alt="png" /></p>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ì˜¤! ë©‹ì§„ë°&hellip;<br />
\(z=wx+b\)ë¡œ ë†“ê³ <br />
\(\hat{y}=\sigma(z)\)ë¡œ ëª¨ë¸ë§í•´ì„œ<br />
\(J(w,b)\) í•¨ìˆ˜ë¥¼ ê·¸ë ¤ë³´ì</p>

<p>ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ìƒˆë¡œìš´ ì†ì‹¤ í•¨ìˆ˜(Loss function)ë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤.</p>
</blockquote>

<h3 id="ë¶„ë¥˜-classification-ë¥¼-ìœ„í•œ-ì†ì‹¤-í•¨ìˆ˜-cross-entropy-loss">ë¶„ë¥˜(Classification)ë¥¼ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜: Cross-entropy Loss</h3>

<p>í•˜ë‚˜ì˜ ë°ì´í„° ì„¸íŠ¸(\(x^{(i)}, y^{(i)}\))ë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì—ì„œ ì–»ì€ ê°’ê³¼ ì‹¤ì œ ê°’ê³¼ì˜ ì°¨ì´(Loss)ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í•´ë³´ì.<br />
ì—¬ê¸°ì„œ \(x^{(i)}\)ëŠ” ië²ˆì§¸ \(x\)ê°’ì´ê³  \(y^{(i)}\)ì€ ië²ˆì§¸ \(y\)ê°’ì´ë‹¤.</p>

<p>ì¼ë‹¨ \(w\)ì™€ \(b\)ëŠ” ì„ì˜ì˜ ê°’ìœ¼ë¡œ ë†“ì. ê·¸ë¦¬ê³  ëª¨ë¸ì— \(x^{(i)}\)ì„ ë„£ê³  ê³„ì‚°í•œ ê²°ê³¼ ê°’ \({\hat y}^{(i)}\)ê³¼ ì‹¤ì œ ê°’ \(y^{(i)}\)ì˜ ì°¨ì´ë¥¼ êµ¬í•œë‹¤. ë¡œì§€ìŠ¤í‹± ëª¨ë¸ë§ì—ì„œëŠ” í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤(Cross-entropy loss)ì„ ì‚¬ìš©í•œë‹¤.</p>

<p>\(L({\hat y}^{(i)}, y^{(i)})=-\bigl (y^{(i)}log({\hat y}^{(i)}) + (1-y^{(i)})log(1-{\hat y}^{(i)})\bigr)\)</p>

<p>ëª¨ë“  ë°ì´í„°(mê°œì˜ ë°ì´í„° ì„¸íŠ¸)ë¡œ ë¶€í„° ì–»ì€ ê²ƒì„ í‰ê·  í•œê²ƒì´ ì†ì‹¤ í•¨ìˆ˜(Loss function)ì´ë‹¤. ì†ì‹¤ í•¨ìˆ˜ëŠ” \(w\)ì™€ \(b\)ì˜ í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.</p>

<p>\({\large J}(w, b) = {1\over m}\sum_{i=1}^m L({\hat y}^{(i)}, y^{(i)}) \\
\\
\hspace{2.9em}= -{1\over {m}}\sum_{i=1}^m [(y^{(i)}log({\hat y}^{(i)})) + (1-y^{(i)})log(1-{\hat y}^{(i)})]\)</p>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ì´ì œë¶€í„°<br />
ì„ í˜• íšŒê·€(Linear regression) ëª¨ë¸ì—ì„œ ì‚¬ìš©ëœ ì†ì‹¤ í•¨ìˆ˜ì¸<br />
í‰ê·  ì œê³± ì˜¤ì°¨ (mean squared error)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ <br />
ê³ ì–‘ì´ëƒ ê°œëƒ í† ë¼ëƒ, Yes or No ë“±ì˜ ë¶„ë¥˜(Classification) ë¬¸ì œë¥¼ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜ë¡œì„œ<br />
ì•„ë˜ì˜ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤.</p>

<p>\({\large J}(w, b) = -{1\over {m}}\sum_{i=1}^m [(y^{(i)}log({\hat y}^{(i)})) + (1-y^{(i)})log(1-{\hat y}^{(i)})]\)</p>

<p>ì—”ì§€ë‹ˆì–´ë¥¼ ê°ˆì•„ì„œ ë§Œë“  ê²ƒì´ë‹ˆ<br />
ìš°ë¦¬ëŠ” ì‚¬ìš©í•˜ê¸°ë§Œ í•˜ë©´ ëœë‹¤</p>
</blockquote>

<p><strong>ì†ì‹¤ í•¨ìˆ˜ (Loss function) ì‹œê°í™”</strong></p>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ìš°ì„  ì†ì‹¤ í•¨ìˆ˜(Loss function)ê°€ ì–´ë–»ê²Œ ìƒê²¨ ë¨¹ì—ˆëŠ”ì§€ ì‚´í´ ë³´ì.<br />
xì¶•ì„ \(w\)ë¡œ ë†“ê³ , yì¶•ì„ \(b\)ë¡œ ë†“ê³ , zì¶•ì„ ì†ì‹¤ í•¨ìˆ˜ \({\large J}(w, b)\)ë¡œ
ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ ë³´ë©´<br />
ì–´ë–»ê²Œ ìµœì†Œê°’ì„ ì°¾ì„ì§€ ê°ì´ ì˜¬ ê²ƒ ê°™ë‹¤.</p>
</blockquote>

<pre><code class="language-python">from sklearn.metrics import log_loss

cross_entropy_loss = True

# W,bì˜ ë²”ìœ„ë¥¼ ê²°ì •í•œë‹¤.
w = np.arange(20, 30, 0.1)
b = np.arange(-4595, -4585, 0.1)

j_loss = []

# ë§¤íŠ¸ë¦­ìŠ¤ë¡œ ë³€í™˜í•œë‹¤.
W, B = np.meshgrid(w, b)

# w, bë¥¼ í•˜ë‚˜ì”© ëŒ€ì‘í•œë‹¤.
for we, be in zip(np.ravel(W), np.ravel(B)):
    z = np.add(np.multiply(we, x), be)
    y_hat = sigmoid(z)
    # Loss function
    if cross_entropy_loss: 
        loss = log_loss(y, y_hat) # Log loss, aka logistic loss or cross-entropy loss.
        j_loss.append(loss)
    else:
        loss = mean_squared_error(y_hat, y) / 2.0 # Mean squred error
        j_loss.append(loss)

# ì†ì‹¤(Loss)ì„ êµ¬í•œë‹¤.
J = np.array(j_loss).reshape(W.shape)

# ì„œí”¼ìŠ¤ ê·¸ë˜í”„ë¥¼ ê·¸ë¦°ë‹¤.
fig = plt.figure()
ax = plt.axes(projection=&quot;3d&quot;)
ax.plot_surface(W, B, J, color='b', alpha=0.5)
ax.set_xlabel('w')
ax.set_ylabel('b')
ax.set_zlabel('J')
plt.show()

</code></pre>

<p><img src="output_21_0.png" alt="png" /></p>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>V ëª¨ì–‘ìœ¼ë¡œ êµ¬ë¶€ëŸ¬ì§„ ëª¨ì–‘ì´ë‹¤!<br />
\(w\)ê°€ 27ê·¼ì²˜ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ê°€ ìµœì†Œê°’ì„ ê°€ì§€ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p>ì†ì‹¤ í•¨ìˆ˜ê°€ ìµœì†Œê°€ ë˜ëŠ” \(w\)ì™€ \(b\)ë¥¼ ë¹ ë¥´ê²Œ ì°¾ê¸° ìœ„í•´ì„œ<br />
ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ<br />
<strong>RMSProp</strong> ì„ ì‚¬ìš©í•œë‹¤.</p>
</blockquote>

<h3 id="rmsprop">RMSProp</h3>

<p>RMSPropì˜ ì›ë¦¬ëŠ” ê¸‰ê²½ì‚¬ì¸ ê²½ìš°ì—ëŠ” ë³´í­ì„ ë‚®ì¶”ì–´ì„œ ê°€ì¥ ì•„ë˜ì¸ì§€ë¥¼ ì„¸ë°€íˆ ì‚´í”¼ê³ , ì™„ë§Œí•œ ê²½ì‚¬ì¸ ê²½ìš°ì—ëŠ” ë³´í­ì„ ë„“í˜€ì„œ ë¹¨ë¦¬ ì§€ë‚˜ê°€ëŠ” ë°©ì‹ì´ë‹¤.<br />
ì´ ë°©ì‹ì€ ë§¤ìš° ë¹ ë¥´ê²Œ ì†ì‹¤ í•¨ìˆ˜ì˜ ìµœì†Œê°’ì„ ì°¾ì„ ìˆ˜ ìˆë‹¤.</p>

<p>\(dw = {\partial {J(w,b)}\over \partial w}\),<br />
\(db = {\partial{J(w,b)}\over \partial b}\)</p>

<p>REPEAT(epoch) {<br />
\(w:=w-\alpha {dw \over {\sqrt {S_{dw}} + \epsilon}}\)</p>

<p>\(b:=b-\alpha {db \over {\sqrt {S_{db}} + \epsilon}}\)<br />
}</p>

<p>\(S_{dw} = \rho S_{dw} + (1-\rho)dw^2\),<br />
\(S_{db} = \rho S_{db} + (1-\rho)db^2\)</p>

<p>\(\alpha=0.001\) : learining rate,<br />
\(\rho=0.9\) : discounting factor,<br />
\(\epsilon=1e-07\) : small value to avoid zero denominator</p>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ì´ê²ƒë„ ì—”ì§€ë‹ˆì–´ë¥¼ ê°ˆì•„ì„œ ë§Œë“  ê²ƒì´ë‹ˆ<br />
ìš°ë¦¬ëŠ” ì‚¬ìš©í•˜ê¸°ë§Œ í•˜ë©´ ëœë‹¤</p>
</blockquote>

<h3 id="ì •ë¦¬">ì •ë¦¬</h3>

<p>ë¡œì§€ìŠ¤í‹± ëª¨ë¸(Logistic model)ì„ ë§Œë“œëŠ” ë°©ë²•ì„ ì •ë¦¬í•´ ë³´ì.</p>

<ol>
<li>\(z=wx+b\) í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤.<br /></li>
<li>\(a = \sigma(z)\) í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤. \(a\)ë¥¼ <strong>í™œì„± í•¨ìˆ˜(activation function)</strong> ë¼ê³  í•œë‹¤.<br /></li>
<li>\(\hat{y} = a\) ì´ë‹¤.</li>
<li>ì†ì‹¤ í•¨ìˆ˜ (Loss function)ë¥¼ ì •ì˜í•œë‹¤. ì—¬ê¸°ì„œëŠ” <strong>í¬ë¡œìŠ¤-ì—”íŠ¸ë¡œí”¼ ì†ì‹¤(cross-entropy loss)</strong> ë¥¼ ì‚¬ìš©í•œë‹¤.<br /></li>
<li>ì˜µí‹°ë§ˆì´ì €(Optimizer)ë¥¼ ì„ íƒí•œë‹¤. ì—¬ê¸°ì„œëŠ” <strong>RMSProp</strong>ì„ ì‚¬ìš©í•œë‹¤.<br /></li>
<li>ë°˜ë³µí•  íšŒìˆ˜(epoch)ë¥¼ ê²°ì •í•œë‹¤.<br /></li>
<li>ì£¼ì–´ì§„ ì¡°ê±´ìœ¼ë¡œ ëª¨ë¸ì„ ìµœì í™”(fit) ì‹œí‚¨ë‹¤.<br /></li>
</ol>

<h2 id="í…ì„œí”Œë¡œìš°-tensorflow-ë¡œ-ëª¨ë¸ë§-modeling">í…ì„œí”Œë¡œìš°(Tensorflow)ë¡œ ëª¨ë¸ë§(Modeling)</h2>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ì¢‹ì•˜ì–´!</p>

<p><strong>ì¼€ë¼ìŠ¤(Keras)</strong>ë¥¼ ì´ìš©í•´ì„œ êµ¬í˜„ì„ í•´ë³´ì!</p>
</blockquote>

<h3 id="ì •ê·œí™”-normalization">ì •ê·œí™” (Normalization)</h3>

<p><strong>ì •ê·œê°’ = (í˜„ì¬ê°’ - ìµœì†Œê°’) / (ìµœëŒ€ê°’-ìµœì†Œê°’)</strong> ìœ¼ë¡œ ì •ê·œí™” í•œë‹¤!</p>

<p>ê·¸ë˜í”„ë¥¼ ë³´ë©´,<br />
ë°ì´í„°ì˜ ëª¨ì–‘ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë©´ì„œë„ \(x\)ì¶•ì˜ ê°’ì´ 0ì—ì„œ 1ì‚¬ì´ë¡œ ë³€í™˜ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.</p>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ì´ì œ ì •ê·œí™”ëŠ”<br />
ì„ íƒì´ ì•„ë‹Œ <strong>í•„ìˆ˜</strong>!</p>
</blockquote>

<pre><code class="language-python">from sklearn import preprocessing

mm_scaler = preprocessing.MinMaxScaler()
X_train = mm_scaler.fit_transform(x)
Y_train = y

plt.scatter(X_train, Y_train)
plt.xlabel('scaled-height')
plt.ylabel('chinchu')
plt.show()
</code></pre>

<p><img src="output_27_0.png" alt="png" /></p>

<h3 id="kerasë¥¼-ê°€ì§€ê³ -ëª¨ë¸ë§-modeling-í•˜ê¸°">Kerasë¥¼ ê°€ì§€ê³  ëª¨ë¸ë§(Modeling)í•˜ê¸°</h3>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>4ì¤„ë¡œ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•˜ë‹¤!</p>

<p>ì¼€ë¼ìŠ¤ ë§Œë§Œì„¸!</p>
</blockquote>

<pre><code class="language-python">from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Activation

# ëª¨ë¸ì„ ì¤€ë¹„í•œë‹¤.
model = Sequential()

# ì…ë ¥ ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ 1ì´ê³  ì¶œë ¥ ê°œìˆ˜ê°€ 1ì¸ y=sigmoid(wx+b)ë¥¼ ìƒì„±í•œë‹¤.
model.add(Dense(1, input_dim=1, activation='sigmoid'))

# Loss funtionê³¼ Optimizerë¥¼ ì„ íƒí•œë‹¤.
model.compile(loss='binary_crossentropy', optimizer='rmsprop') 

# epochsë§Œí¼ ë°˜ë³µí•´ì„œ ì†ì‹¤ê°’ì´ ìµœì €ê°€ ë˜ë„ë¡ ëª¨ë¸ì„ í›ˆë ¨í•œë‹¤.
hist = model.fit(X_train, Y_train, epochs=10000, batch_size=20, verbose=0) 
</code></pre>

<pre><code>WARNING: Logging before flag parsing goes to stderr.
W0901 10:08:24.463013 140371975219008 deprecation.py:506] From /home/dataman/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0901 10:08:24.482560 140371975219008 deprecation.py:323] From /home/dataman/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
</code></pre>

<h3 id="ì†ì‹¤ê°’ì˜-ë³€í™”ë¥¼-ê·¸ë˜í”„ë¡œ-í™•ì¸">ì†ì‹¤ê°’ì˜ ë³€í™”ë¥¼ ê·¸ë˜í”„ë¡œ í™•ì¸</h3>

<pre><code class="language-python">plt.plot(hist.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
</code></pre>

<p><img src="output_31_0.png" alt="png" /></p>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ë¡ ì†ì‹¤(Loss)ì´ 0ì— ê°€ê¹ê²Œ ëœë‹¤.<br />
ë‚˜ì´ìŠ¤!</p>
</blockquote>

<h3 id="w-ì™€-b-ê°’ì„-í™•ì¸">\(w\)ì™€ \(b\)ê°’ì„ í™•ì¸</h3>

<pre><code class="language-python">w, b = model.get_weights()
w =  w[0][0]
b = b[0]
print('w: ', w)
print('b: ', b)
</code></pre>

<pre><code>w:  14.712823
b:  -7.5659995
</code></pre>

<h3 id="ê·¸ë˜í”„ë¡œ-í™•ì¸">ê·¸ë˜í”„ë¡œ í™•ì¸</h3>

<pre><code class="language-python">x_scale = mm_scaler.transform(x)
plt.scatter(x_scale, y)
plt.plot(x_scale, sigmoid(w*np.array(x_scale)+b), 'r')
plt.xlabel('scaled-height')
plt.ylabel('chinchu')
plt.show()
</code></pre>

<p><img src="output_36_0.png" alt="png" /></p>

<h2 id="í•´ê²°-solution">í•´ê²° (Solution)</h2>

<p>âš™ï¸ ì—”ì§€ë‹ˆì–´</p>

<blockquote>
<p>ê³ ê°ë‹˜~ ì›í•˜ì‹œëŠ” ì†”ë£¨ì…˜ì…ë‹ˆë‹¤.<br />
input_heightì— ì›í•˜ì‹œëŠ” í‚¤ë¥¼ ì…ë ¥í•˜ì‹œë©´<br />
&lsquo;ì¹œì¶”&rsquo;ê°€ ë  í™•ë¥ ì„ ì•Œë ¤ ì¤ë‹ˆë‹¤.</p>
</blockquote>

<pre><code class="language-python">input_height = 178.0

input_x = mm_scaler.transform(np.array([input_height]).reshape(-1, 1))
predict = model.predict(input_x)

print('ì¹œì¶”ê°€ ë  í™•ë¥ ì€ {:.1f}% ì…ë‹ˆë‹¤.'.format(predict[0][0]*100))
</code></pre>

<pre><code>ì¹œì¶”ê°€ ë  í™•ë¥ ì€ 93.9% ì…ë‹ˆë‹¤.
</code></pre>
		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/" rel="tag">ë”¥ëŸ¬ë‹</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80/" rel="tag">ë¡œì§€ìŠ¤í‹±íšŒê·€</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C/" rel="tag">ì‹œê·¸ëª¨ì´ë“œ</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/%ED%99%9C%EC%84%B1-%ED%95%A8%EC%88%98/" rel="tag">í™œì„± í•¨ìˆ˜</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/cross-entropy-loss/" rel="tag">Cross entropy loss</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/rmsprop/" rel="tag">RMSProp</a></li>
	</ul>
</div>
		</footer>
	</article>
</main>


<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/post/linear_regression/" rel="prev"><span class="post-nav__caption">Â«&thinsp;Previous</span><p class="post-nav__post-title">ì„ í˜• íšŒê·€ (Linear Regression)</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/post/neural_network/" rel="next"><span class="post-nav__caption">Next&thinsp;Â»</span><p class="post-nav__post-title">ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ (Neural Network)</p></a>
	</div>
</nav>
<script src="https://utteranc.es/client.js"
        repo="skettee/blog-comments"
        issue-term="pathname"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2019 ë§‰ë¼ë¨¹ì€ ì—”ì§€ë‹ˆì–´ì˜ ë¨¸ì‹ ëŸ°ë‹ ê°œë°œ.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>